{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def formating(tX) :\n",
    "    tX_final =tX    \n",
    "    number = np.zeros(tX.shape[0])\n",
    "    for j in range(tX.shape[1]) :\n",
    "        r = [index for index,value in enumerate(tX[:,j]) if value != -999.]\n",
    "        x = tX[r,j]\n",
    "        median = np.median(x)\n",
    "        #print(median)\n",
    "        for i in range(tX.shape[0]) :\n",
    "            if tX[i][j]==-999. :\n",
    "                number[i]+=1\n",
    "                tX_final[i,j]= median\n",
    "    tX_final = np.c_[tX,number]  \n",
    "    \n",
    "    return tX_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tX_final = formating(tX) \n",
    "mean = np.mean(tX_final,axis=0)\n",
    "#print(mean.shape)\n",
    "var  = np.std(tX_final)\n",
    "tX_final = (tX_final-mean[0])/ var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(9/9): loss=0.4999893444180495\n"
     ]
    }
   ],
   "source": [
    "# Gradient Descent.\n",
    "import gradient_descent as gd\n",
    "def least_squares_GD(y, tX, initial_w, max_iters, gamma) :\n",
    "\n",
    " return gd.gradient_descent(y, tX, initial_w, max_iters, gamma)\n",
    "\n",
    "initial_w = np.zeros(tX_final.shape[1]) # initialisation of initial weights\n",
    "max_iters = 10 # max iter number\n",
    "gamma = 3e-7 # gradient descent speed \n",
    "[gradient_losses,gradient_weights]=least_squares_GD(y, tX_final, initial_w, max_iters, gamma) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(999/999): loss=0.41171238552481154\n",
      "[ 0.01070574 -0.10155941  0.00074656  0.06059049  0.0176896   0.0561244\n",
      "  0.01584445  0.01735041 -0.0005775   0.02175829  0.01522517  0.01975283\n",
      "  0.01733401  0.05276419  0.01703529  0.01705712 -0.00497349  0.01709106\n",
      "  0.01726714 -0.00445455  0.01726725  0.00940706  0.01737613  0.00508787\n",
      "  0.01705951  0.01713509 -0.00365964  0.01718606  0.01719399  0.00825059\n",
      "  0.01267637]\n"
     ]
    }
   ],
   "source": [
    "# Stochastic Gradient Descent.\n",
    "import stochastic_gradient_descent as sgd\n",
    "def least_squares_SGD(y, tx, initial_w, batch_size, max_epochs, gamma) :\n",
    "\n",
    " return sgd.stochastic_gradient_descent(y, tx, initial_w, batch_size, max_epochs, gamma)\n",
    "\n",
    "initial_w = np.zeros(tX_final.shape[1]) # initialisation of initial weights\n",
    "max_iters = 1000 # max iter number\n",
    "gamma = 1e-3 # gradient descent speed  \n",
    "batch_size = 30\n",
    "[sto_Losses , sto_gradient_weight] =least_squares_SGD(y,tX_final,initial_w,batch_size,max_iters,gamma )\n",
    "print(sto_gradient_weight[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.898609118601\n"
     ]
    }
   ],
   "source": [
    "#Least squares.\n",
    "import least_squares as ls\n",
    "import costs as co\n",
    "from build_polynomial import *\n",
    "\n",
    "def least_squares_vanilla(y,tx):\n",
    "    degree = 5\n",
    "    tX = build_poly(tx, degree)\n",
    "    weights = ls.least_squares(y ,tX)\n",
    "    rmse = np.sqrt(2*co.compute_loss(y,tX,weights))\n",
    "    print(rmse)\n",
    "    return rmse , weights\n",
    "\n",
    "[vanilla_losses , vanilla_weights] = least_squares_vanilla(y,tX_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### ridge regression\n",
    "import least_squares_ridge as lsr\n",
    "import costs as co\n",
    "from build_polynomial import *\n",
    "def ridge_regression_demo(y, tx,lamb,degree):\n",
    "     # define parameter\n",
    "     tX = build_poly(tx, degree)\n",
    "     weight =  lsr.ridge_regression(y , tX , lamb)\n",
    "    \n",
    "     rmse_tr = np.sqrt(2*(co.compute_loss(y,tX,weight)))\n",
    "     \n",
    "     print(\"Training RMSE={tr:.3f}\".format(tr=rmse_tr))\n",
    "     return weight\n",
    "\n",
    "#lambdas = np.linspace(0, 1e-10, num=10)\n",
    "#print(lambdas)\n",
    "#for lambda_ in lambdas :\n",
    "#   print(lambda_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Barry/Google Drive/master 3/MLPC/PCML-Project/logistique_regression.py:21: RuntimeWarning: overflow encountered in exp\n",
      "  return sum(np.log(1+np.exp(np.dot(tx,w)))-np.multiply(y,(np.dot(tx,w))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=[ 173286.7951405]\n",
      "Current iteration=1, the loss=[ 165177.38649192]\n",
      "Current iteration=2, the loss=[ 161923.61398003]\n",
      "Current iteration=3, the loss=[ 160293.34627148]\n",
      "Current iteration=4, the loss=[ 159265.54093757]\n",
      "Current iteration=5, the loss=[ 157071.37621424]\n",
      "Current iteration=6, the loss=[ 155786.20026606]\n",
      "Current iteration=7, the loss=[ 154962.34803675]\n",
      "Current iteration=8, the loss=[ 153404.64851783]\n",
      "Current iteration=9, the loss=[ 152723.94976519]\n",
      "Current iteration=10, the loss=[ 152238.81444176]\n",
      "Current iteration=11, the loss=[ 152146.84836787]\n",
      "Current iteration=12, the loss=[ 151966.74514045]\n",
      "Current iteration=13, the loss=[ 150648.64765626]\n",
      "Current iteration=14, the loss=[ 150448.28248285]\n",
      "Current iteration=15, the loss=[ 150346.21840311]\n",
      "Current iteration=16, the loss=[ 150282.69572626]\n",
      "Current iteration=17, the loss=[ 150139.76112637]\n",
      "Current iteration=18, the loss=[ 148909.03912994]\n",
      "Current iteration=19, the loss=[ 148794.69999338]\n",
      "Current iteration=20, the loss=[ 148728.99352469]\n",
      "Current iteration=21, the loss=[ 148596.43895453]\n",
      "Current iteration=22, the loss=[ 147552.92393355]\n",
      "Current iteration=23, the loss=[ 147422.44474393]\n",
      "Current iteration=24, the loss=[ 147352.52061393]\n",
      "Current iteration=25, the loss=[ 147227.2708856]\n",
      "Current iteration=26, the loss=[ 146351.81255633]\n",
      "Current iteration=27, the loss=[ 146203.87843142]\n",
      "Current iteration=28, the loss=[ 146128.96338742]\n",
      "Current iteration=29, the loss=[ 146082.97040925]\n",
      "Current iteration=30, the loss=[ 145981.79066735]\n",
      "Current iteration=31, the loss=[ 145118.25930496]\n",
      "Current iteration=32, the loss=[ 145038.26562402]\n",
      "Current iteration=33, the loss=[ 144991.87153835]\n",
      "Current iteration=34, the loss=[ 144897.94158757]\n",
      "Current iteration=35, the loss=[ 144147.04180473]\n",
      "Current iteration=36, the loss=[ 144062.80302895]\n",
      "Current iteration=37, the loss=[ 144016.12170975]\n",
      "Current iteration=38, the loss=[ 143928.5458991]\n",
      "Current iteration=39, the loss=[ 143274.85364384]\n",
      "Current iteration=40, the loss=[ 143187.80833024]\n",
      "Current iteration=41, the loss=[ 143141.19255251]\n",
      "Current iteration=42, the loss=[ 143059.32620992]\n",
      "Current iteration=43, the loss=[ 142487.49232927]\n",
      "Current iteration=44, the loss=[ 142400.08749763]\n",
      "Current iteration=45, the loss=[ 142354.3581769]\n",
      "Current iteration=46, the loss=[ 142325.50272679]\n",
      "Current iteration=47, the loss=[ 142260.43513208]\n",
      "Current iteration=48, the loss=[ 141688.13144854]\n",
      "Current iteration=49, the loss=[ 141644.6234363]\n"
     ]
    }
   ],
   "source": [
    "from logistique_regression import *\n",
    "\n",
    "def logistic_regression_newton_method_demo(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 5000\n",
    "    alpha = 1e-28\n",
    "    threshold = 0.01\n",
    "    lambda_ = 0.1\n",
    "    losses = []\n",
    "    degree = 1\n",
    "    # build tx\n",
    "    #tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    tx = build_poly(x, degree)\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "    y = (1+y)/2\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        \n",
    "        loss, w = learning_by_newton_method(y.reshape(y.shape[0],1), tx, w, alpha)\n",
    "        # log info\n",
    "        if iter % 1 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criteria\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    # visualization\n",
    "\n",
    "    return loss , w\n",
    "\n",
    "loss , w = logistic_regression_newton_method_demo(y, tX_final )\n",
    "print (w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'Test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238, 31)\n"
     ]
    }
   ],
   "source": [
    "tX_final_test = tX_test\n",
    "#tX_final_test =(tX_final_test - mean)/var\n",
    "tX_final_test = formating(tX_final_test) \n",
    "tX_final_test = (tX_final_test-mean[0])/ var\n",
    "print(tX_final_test.shape)\n",
    "\n",
    "OUTPUT_PATH = 'least_square_G.csv' # TODO: fill in desired name of output file for submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(gradient_weights)\n",
    "y_pred = np.zeros((tX_test.shape[0],1))\n",
    "print(y_pred.shape)\n",
    "lambda_= 0\n",
    "for degree in range(5,15) :\n",
    "    print(degree)\n",
    "    tX_final_degree = build_poly(tX_final_test,degree)\n",
    "    \n",
    "    weights = ridge_regression_demo(y,tX_final,lambda_,degree)\n",
    "    p =  predict_labels(weights, tX_final_degree)\n",
    "    y_pred =y_pred+ p.reshape((p.shape[0],1))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#y_pred = 1*np.sign((y_pred/10))\n",
    "tX_final_test = build_poly(tX_final_test,11)\n",
    "\n",
    "y_pred =  predict_labels(w, tX_final_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
