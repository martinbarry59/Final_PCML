{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = r'../train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def formating(tX) :\n",
    "    tX_final =tX    \n",
    "    number = np.zeros(tX.shape[0])\n",
    "    index = []\n",
    "    \n",
    "    for j in range(tX.shape[1]) :\n",
    "        r = [index for index,value in enumerate(tX[:,j]) if value != -999.]\n",
    "        x = tX[r,j]\n",
    "        median = np.median(x)\n",
    "        #print(median)\n",
    "        if median == median :\n",
    "            index.append(j)\n",
    "        for i in range(tX.shape[0]) :\n",
    "            if tX[i][j]!=tX[i][j] :\n",
    "                number[i]+=1\n",
    "                tX_final[i,j]= median    \n",
    "    print(index)\n",
    "    tX_final = tX_final[:,index]\n",
    "    return tX_final , index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from build_polynomial import *\n",
    "\n",
    "tX_final ,index = formating(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Gradient Descent.\n",
    "import gradient_descent as gd\n",
    "def least_squares_GD(y, tX, initial_w, max_iters, gamma) :\n",
    "\n",
    " return gd.gradient_descent(y, tX, initial_w, max_iters, gamma)\n",
    "\n",
    "max_iters = 10 # max iter number\n",
    "gamma = 3e-37 # gradient descent speed , not used here as gamma updated by backtracking\n",
    "tX_final_gd = build_poly(tX_final, 5)\n",
    "initial_w = np.zeros(tX_final_gd.shape[1]) # initialisation of initial weights\n",
    "[gradient_losses,gradient_weights]=least_squares_GD(y, tX_final_gd, initial_w, max_iters, gamma) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent.\n",
    "import stochastic_gradient_descent as sgd\n",
    "def least_squares_SGD(y, tx, initial_w, batch_size, max_epochs, gamma) :\n",
    "\n",
    " return sgd.stochastic_gradient_descent(y, tx, initial_w, batch_size, max_epochs, gamma)\n",
    "\n",
    "initial_w = np.zeros(tX_final.shape[1]) # initialisation of initial weights\n",
    "max_iters = 1000 # max iter number\n",
    "gamma = 1e-8 # gradient descent speed  \n",
    "batch_size = 30\n",
    "[sto_Losses , sto_gradient_weight] =least_squares_SGD(y,tX_final,initial_w,batch_size,max_iters,gamma )\n",
    "print(sto_gradient_weight[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Least squares.\n",
    "import least_squares as ls\n",
    "import costs as co\n",
    "\n",
    "def least_squares_vanilla(y,tx):\n",
    "    degree = 1\n",
    "    tX = build_poly(tx, degree)\n",
    "    weights = ls.least_squares(y ,tX)\n",
    "    rmse = np.sqrt(2*co.compute_loss(y,tX,weights))\n",
    "    print(rmse)\n",
    "    return rmse , weights\n",
    "\n",
    "[vanilla_losses , vanilla_weights] = least_squares_vanilla(y,tX_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "index0 = tX[:,22] == 0\n",
    "tX0 = tX[index0,:]\n",
    "tX0 = np.delete(tX0,22,1)\n",
    "index1 = tX[:,22] == 1\n",
    "tX1 = tX[index1,:]\n",
    "tX1 = np.delete(tX1,22,1)\n",
    "index2 = tX[:,22] == 2\n",
    "tX2 = tX[index2,:]\n",
    "tX2 = np.delete(tX2,22,1)\n",
    "index3 = tX[:,22] == 3\n",
    "tX3 = tX[index3,:]\n",
    "tX3 = np.delete(tX3,22,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search(y,tx,lambda_,degree):\n",
    "    los = np.zeros((len(lambda_),len(degree)))\n",
    "    for i in range(0,len(los[0])):\n",
    "        for j in range(0,len(los[1])):\n",
    "            weight,losses = ridge_regression_demo(y, tx,lambda_[i],degree[j])\n",
    "            los[i,j] = losses\n",
    "    return los\n",
    "def get_best_parameters(lambda_, degree, losses):\n",
    "    \"\"\"Get the best w from the result of grid search.\"\"\"\n",
    "    min_row, min_col = np.unravel_index(np.argmin(losses), losses.shape)\n",
    "    return losses[min_row, min_col], lambda_[min_row], degree[min_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tX0_final, index_final_0 = formating(tX0) \n",
    "tX1_final, index_final_1 = formating(tX1) \n",
    "tX2_final, index_final_2 = formating(tX2) \n",
    "tX3_final, index_final_3 = formating(tX3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##### ridge regression\n",
    "import least_squares_ridge as lsr\n",
    "import costs as co\n",
    "from build_polynomial import *\n",
    "def ridge_regression_demo(y, tx,lamb,degree):\n",
    "     # define parameter\n",
    "     tX = build_poly(tx, degree)\n",
    "     weight =  lsr.ridge_regression(y , tX , lamb)\n",
    "    \n",
    "     loss = np.sqrt(2*(co.compute_loss(y,tX,weight)))\n",
    "     \n",
    "     print(\"Training RMSE={tr:.3f}\".format(tr=loss))\n",
    "     return weight ,loss\n",
    "lambdas = np.logspace(-10,1,10)\n",
    "degrees = [5,6,7,8,9,10,11,12,13,14]\n",
    "\n",
    "#need to put this in the final .py, it's juts to long to be efficient so only run one time, then parameters are put by hands\n",
    "\n",
    "\n",
    "#losses0 = grid_search(y[index0],tX0_final,lambdas,degrees)\n",
    "#value0,lambda_0, degree_0 = get_best_parameters(lambdas,degrees,losses0)\n",
    "#print(value0,lambda_0,degree_0)\n",
    "\n",
    "#losses1 = grid_search(y[index1],tX1_final,lambdas,degrees)\n",
    "#value1,lambda_1, degree_1 = get_best_parameters(lambdas,degrees,losses1)\n",
    "#print(value1,lambda_1,degree_1)\n",
    "\n",
    "#losses2 = grid_search(y[index2],tX2_final,lambdas,degrees)\n",
    "#value2,lambda_2, degree_2= get_best_parameters(lambdas,degrees,losses2)\n",
    "#print(value2,lambda_2,degree_2)\n",
    "\n",
    "#losses3 = grid_search(y[index3],tX3_final,lambdas,degrees)\n",
    "#value3,lambda_3, degree_3 = get_best_parameters(lambdas,degrees,losses3)\n",
    "#print(value3,lambda_3,degree_3)\n",
    "lamb0 =1.6681005372e-09\n",
    "degree0 = 6\n",
    "weight0 , loss0 = ridge_regression_demo(y[index0], tX0_final,lamb0,degree0)\n",
    "lamb1 = 7.74263682681e-06\n",
    "degree1 = 10\n",
    "weight1 , loss1 = ridge_regression_demo(y[index1], tX1_final,lamb1,degree1)\n",
    "lamb2 = 2.78255940221e-08\n",
    "degree2 = 11\n",
    "weight2 , loss2 = ridge_regression_demo(y[index2], tX2_final,lamb2,degree2)\n",
    "lamb3 = 4.64158883361e-07\n",
    "degree3 = 11\n",
    "weight3 , loss3  = ridge_regression_demo(y[index3], tX3_final,lamb3,degree3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from logistique_regression import *\n",
    "\n",
    "def logistic_regression_newton_method_demo(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 5000\n",
    "    alpha = 1e-28\n",
    "    threshold = 0.1\n",
    "    lambda_ = 0.1\n",
    "    losses = []\n",
    "    degree = 1\n",
    "    # build tx\n",
    "    #tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "    tx = build_poly(x, degree)\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "    y = (1+y)/2\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        \n",
    "        loss, w = learning_by_newton_method(y.reshape(y.shape[0],1), tx, w, alpha)\n",
    "        # log info\n",
    "        if iter % 10 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criteria\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    # visualization\n",
    "\n",
    "    return loss , w\n",
    "\n",
    "loss , w = logistic_regression_newton_method_demo(y, tX_final )\n",
    "print (w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../Test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BEST SCORE PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index0 = tX_test[:,22] == 0\n",
    "tX0 = tX_test[index0,:]\n",
    "tX0 = np.delete(tX0,22,1)\n",
    "index1 = tX_test[:,22] == 1\n",
    "tX1 = tX_test[index1,:]\n",
    "tX1 = np.delete(tX1,22,1)\n",
    "index2 = tX_test[:,22] == 2\n",
    "tX2 = tX_test[index2,:]\n",
    "tX2 = np.delete(tX2,22,1)\n",
    "index3 = tX_test[:,22] == 3\n",
    "tX3 = tX_test[index3,:]\n",
    "tX3 = np.delete(tX3,22,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tX0_final, index_final_0 = formating(tX0) \n",
    "tX1_final, index_final_1 = formating(tX1) \n",
    "tX2_final, index_final_2 = formating(tX2) \n",
    "tX3_final, index_final_3 = formating(tX3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tX0_final_test =  build_poly(tX0_final, degree0)\n",
    "ypred0 = predict_labels(weight0, tX0_final_test)\n",
    "tX1_final_test =  build_poly(tX1_final, degree1)\n",
    "ypred1 = predict_labels(weight1, tX1_final_test)\n",
    "tX2_final_test =  build_poly(tX2_final, degree2)\n",
    "ypred2 = predict_labels(weight2, tX2_final_test)\n",
    "tX3_final_test =  build_poly(tX3_final, degree3)\n",
    "ypred3 = predict_labels(weight3, tX3_final_test)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = np.zeros((tX_test.shape[0]))\n",
    "y_pred[index0]=ypred0.reshape(ypred0.shape[0],1)\n",
    "y_pred[index1]=ypred1.reshape(ypred1.shape[0],1)\n",
    "y_pred[index2]=ypred2.reshape(ypred2.shape[0],1)\n",
    "y_pred[index3]=ypred3.reshape(ypred3.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#y_pred = 1*np.sign((y_pred/10))\n",
    "#tX_final_test = build_poly(tX_final_test,11)\n",
    "OUTPUT_PATH = 'diffweight.csv' \n",
    "#y_pred =  predict_labels(w, tX_final_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tX_final_test, index_final_test = formating(tX_test) \n",
    "ypred = predict_labels(weight, tX_final_test)\n",
    "OUTPUT_PATH = 'prediction.csv' \n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
