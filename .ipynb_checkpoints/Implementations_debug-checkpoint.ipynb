{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = r'../train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n"
     ]
    }
   ],
   "source": [
    "import implementations as im\n",
    "tX_final ,index = im.formating(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.81520424476945e-33\n",
      "Gradient Descent(0/9): loss=0.5\n",
      "1.2717347482564833e-32\n",
      "Gradient Descent(1/9): loss=0.49652700044982856\n",
      "1.2717347482564833e-32\n",
      "Gradient Descent(2/9): loss=0.48920232156605736\n",
      "4.2391158275216116e-32\n",
      "Gradient Descent(3/9): loss=0.48289384138574587\n",
      "3.81520424476945e-33\n",
      "Gradient Descent(4/9): loss=0.4673124648135486\n",
      "1.2717347482564833e-32\n",
      "Gradient Descent(5/9): loss=0.46530198497592534\n",
      "1.2717347482564833e-32\n",
      "Gradient Descent(6/9): loss=0.46179412406286424\n",
      "1.2717347482564833e-32\n",
      "Gradient Descent(7/9): loss=0.4588082160477803\n",
      "4.2391158275216116e-32\n",
      "Gradient Descent(8/9): loss=0.4562188861988009\n",
      "3.81520424476945e-33\n",
      "Gradient Descent(9/9): loss=0.4496426589503343\n"
     ]
    }
   ],
   "source": [
    "# Least Squares GD\n",
    "max_iters = 10 # max iter number\n",
    "gamma = 3e-37 # gradient descent speed , not used here as gamma updated by backtracking\n",
    "tX_final_gd = im.build_poly(tX_final, 5)\n",
    "initial_w = np.zeros(tX_final_gd.shape[1]) # initialisation of initial weights\n",
    "weight, loss = im.least_squares_GD(y, tX_final_gd, initial_w, max_iters, gamma) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(999/999): loss=0.44386681613424916\n",
      "[  4.05871775e-04  -1.25661727e-04  -3.67879389e-05  -9.76611851e-06\n",
      "  -1.01755935e-05   1.20946673e-04  -1.33761925e-05  -5.02986504e-07\n",
      "  -2.27997129e-05  -1.07530674e-04  -2.75240541e-06   1.16355404e-06\n",
      "  -1.04943634e-05   3.02276529e-05  -4.04711093e-07   2.09518431e-07\n",
      "  -4.72070576e-05   2.45621574e-07  -1.35294862e-07  -4.13194545e-05\n",
      "   2.57031628e-07  -1.42176730e-04  -1.36809594e-06  -7.20737428e-06\n",
      "   2.64815776e-05   2.68302393e-05  -4.41173826e-05  -1.03138557e-05\n",
      "  -1.05403783e-05  -9.05512198e-05]\n"
     ]
    }
   ],
   "source": [
    "# Least Squares SGD\n",
    "initial_w = np.zeros(tX_final.shape[1]) # initialisation of initial weights\n",
    "max_iters = 1000 # max iter number\n",
    "gamma = 1e-8 # gradient descent speed  \n",
    "weight, loss = im.least_squares_SGD(y, tX_final, initial_w, max_iters, gamma)\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  8.03494364e-05  -7.20202264e-03  -6.05417274e-03  -5.47559082e-04\n",
      "  -1.93874699e-02   4.73451612e-04  -2.60379063e-02   3.25106299e-01\n",
      "  -3.80779908e-05  -2.72809161e+00  -2.21220141e-01   9.50794099e-02\n",
      "   6.40351632e-02   2.73635128e+00  -3.31801028e-04  -9.54325157e-04\n",
      "   2.74110803e+00  -5.34165388e-04   9.73499007e-04   3.69225050e-03\n",
      "   3.54487064e-04  -5.43344624e-04  -3.30448035e-01  -1.40800496e-03\n",
      "   8.31432780e-04   1.02117281e-03  -1.68047419e-03  -5.83664806e-03\n",
      "  -1.11088010e-02   2.72855154e+00]\n",
      "0.824241238324\n"
     ]
    }
   ],
   "source": [
    "# Least Squares\n",
    "weight, loss = im.least_squares(y,tX_final)\n",
    "print(weight)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loris/anaconda3/lib/python3.5/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n",
      "/home/loris/anaconda3/lib/python3.5/site-packages/numpy/core/_methods.py:70: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 28]\n",
      "[0, 1, 2, 3, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 28]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]\n",
      "Training RMSE=0.665\n",
      "Training RMSE=0.739\n",
      "Training RMSE=0.678\n",
      "Training RMSE=0.740\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression\n",
    "def ridge_regression_demo(y, tx,lamb,degree):\n",
    "    # define parameter\n",
    "    tX = im.build_poly(tx, degree)\n",
    "    weight, loss =  im.ridge_regression(y , tX , lamb)\n",
    "    \n",
    "    print(\"Training RMSE={tr:.3f}\".format(tr=loss))\n",
    "    return weight ,loss\n",
    "\n",
    "index0 = tX[:,22] == 0\n",
    "tX0 = tX[index0,:]\n",
    "tX0 = np.delete(tX0,22,1)\n",
    "index1 = tX[:,22] == 1\n",
    "tX1 = tX[index1,:]\n",
    "tX1 = np.delete(tX1,22,1)\n",
    "index2 = tX[:,22] == 2\n",
    "tX2 = tX[index2,:]\n",
    "tX2 = np.delete(tX2,22,1)\n",
    "index3 = tX[:,22] == 3\n",
    "tX3 = tX[index3,:]\n",
    "tX3 = np.delete(tX3,22,1)\n",
    "\n",
    "tX0_final, index_final_0 = im.formating(tX0) \n",
    "tX1_final, index_final_1 = im.formating(tX1) \n",
    "tX2_final, index_final_2 = im.formating(tX2) \n",
    "tX3_final, index_final_3 = im.formating(tX3) \n",
    "\n",
    "lambdas = np.logspace(-10,1,10)\n",
    "degrees = [5,6,7,8,9,10,11,12,13,14]\n",
    "\n",
    "\n",
    "lamb0 =1.6681005372e-09\n",
    "degree0 = 6\n",
    "weight0 , loss0 = ridge_regression_demo(y[index0], tX0_final,lamb0,degree0)\n",
    "lamb1 = 7.74263682681e-06\n",
    "degree1 = 10\n",
    "weight1 , loss1 = ridge_regression_demo(y[index1], tX1_final,lamb1,degree1)\n",
    "lamb2 = 2.78255940221e-08\n",
    "degree2 = 11\n",
    "weight2 , loss2 = ridge_regression_demo(y[index2], tX2_final,lamb2,degree2)\n",
    "lamb3 = 4.64158883361e-07\n",
    "degree3 = 11\n",
    "weight3 , loss3  = ridge_regression_demo(y[index3], tX3_final,lamb3,degree3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=173286.79514050332\n",
      "Current iteration=10, the loss=156481.31103803436\n",
      "Current iteration=20, the loss=155597.6582934794\n",
      "Current iteration=30, the loss=154867.8476989706\n",
      "Current iteration=40, the loss=154246.9000077165\n",
      "Current iteration=50, the loss=153710.2042982996\n",
      "Current iteration=60, the loss=153240.5324009513\n",
      "Current iteration=70, the loss=152825.4108192396\n",
      "Current iteration=80, the loss=152455.51124146476\n",
      "Current iteration=90, the loss=152123.65027883483\n",
      "Current iteration=100, the loss=151824.15585343403\n",
      "Current iteration=110, the loss=151552.4558497049\n",
      "Current iteration=120, the loss=151304.8032594236\n",
      "Current iteration=130, the loss=151078.08668663035\n",
      "Current iteration=140, the loss=150869.69547545922\n",
      "Current iteration=150, the loss=150677.4207486574\n",
      "Current iteration=160, the loss=150499.38076853016\n",
      "Current iteration=170, the loss=150333.96328857966\n",
      "Current iteration=180, the loss=150179.78013858545\n",
      "Current iteration=190, the loss=150035.6308679878\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "max_iters = 200\n",
    "gamma = 1e-12\n",
    "init_w = np.zeros(tX_final.shape[1])\n",
    "w, loss = im.logistic_regression(y, tX_final, init_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=173286.79514050332\n",
      "Current iteration=10, the loss=156481.31103803436\n",
      "Current iteration=20, the loss=155597.6582934794\n",
      "Current iteration=30, the loss=154867.8476989706\n",
      "Current iteration=40, the loss=154246.9000077165\n",
      "Current iteration=50, the loss=153710.2042982996\n",
      "Current iteration=60, the loss=153240.5324009513\n",
      "Current iteration=70, the loss=152825.4108192396\n",
      "Current iteration=80, the loss=152455.51124146476\n",
      "Current iteration=90, the loss=152123.65027883483\n",
      "Current iteration=100, the loss=151824.15585343403\n",
      "Current iteration=110, the loss=151552.4558497049\n",
      "Current iteration=120, the loss=151304.8032594236\n",
      "Current iteration=130, the loss=151078.08668663035\n",
      "Current iteration=140, the loss=150869.69547545922\n",
      "Current iteration=150, the loss=150677.4207486574\n",
      "Current iteration=160, the loss=150499.38076853016\n",
      "Current iteration=170, the loss=150333.96328857966\n",
      "Current iteration=180, the loss=150179.78013858545\n",
      "Current iteration=190, the loss=150035.6308679878\n",
      "Current iteration=200, the loss=149900.47326438475\n",
      "Current iteration=210, the loss=149773.39920061885\n",
      "Current iteration=220, the loss=149653.61468382835\n",
      "Current iteration=230, the loss=149540.4232644727\n",
      "Current iteration=240, the loss=149433.21216227708\n",
      "Current iteration=250, the loss=149331.44060882548\n",
      "Current iteration=260, the loss=149234.6300119087\n",
      "Current iteration=270, the loss=149142.3556258335\n",
      "Current iteration=280, the loss=149054.23947309252\n",
      "Current iteration=290, the loss=148969.94431022985\n",
      "Current iteration=300, the loss=148889.1684684369\n",
      "Current iteration=310, the loss=148811.64142934245\n",
      "Current iteration=320, the loss=148737.12002085967\n",
      "Current iteration=330, the loss=148665.38513738863\n",
      "Current iteration=340, the loss=148596.23890482762\n",
      "Current iteration=350, the loss=148529.502223831\n",
      "Current iteration=360, the loss=148465.01263567674\n",
      "Current iteration=370, the loss=148402.62246392824\n",
      "Current iteration=380, the loss=148342.19719240288\n",
      "Current iteration=390, the loss=148283.6140462816\n",
      "Current iteration=400, the loss=148226.76074802165\n",
      "Current iteration=410, the loss=148171.53442426518\n",
      "Current iteration=420, the loss=148117.84064332553\n",
      "Current iteration=430, the loss=148065.5925658967\n",
      "Current iteration=440, the loss=148014.7101941382\n",
      "Current iteration=450, the loss=147965.11970649246\n",
      "Current iteration=460, the loss=147916.75286718982\n",
      "Current iteration=470, the loss=147869.54650117646\n",
      "Current iteration=480, the loss=147823.44202628266\n",
      "Current iteration=490, the loss=147778.38503570788\n"
     ]
    }
   ],
   "source": [
    "# Reg Logistic Regression\n",
    "reg_w, reg_loss = im.reg_logistic_regression(y,tX_final, 0.5, init_w, 500, 1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import run"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
