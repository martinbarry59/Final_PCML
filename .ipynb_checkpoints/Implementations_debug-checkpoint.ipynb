{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = r'../train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n"
     ]
    }
   ],
   "source": [
    "import implementations as im\n",
    "tX_final ,index = im.formating(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.81520424476945e-33\n",
      "Gradient Descent(0/9): loss=0.5\n",
      "1.2717347482564833e-32\n",
      "Gradient Descent(1/9): loss=0.49652700044982856\n",
      "1.2717347482564833e-32\n",
      "Gradient Descent(2/9): loss=0.48920232156605736\n",
      "4.2391158275216116e-32\n",
      "Gradient Descent(3/9): loss=0.48289384138574587\n",
      "3.81520424476945e-33\n",
      "Gradient Descent(4/9): loss=0.4673124648135486\n",
      "1.2717347482564833e-32\n",
      "Gradient Descent(5/9): loss=0.46530198497592534\n",
      "1.2717347482564833e-32\n",
      "Gradient Descent(6/9): loss=0.46179412406286424\n",
      "1.2717347482564833e-32\n",
      "Gradient Descent(7/9): loss=0.4588082160477803\n",
      "4.2391158275216116e-32\n",
      "Gradient Descent(8/9): loss=0.4562188861988009\n",
      "3.81520424476945e-33\n",
      "Gradient Descent(9/9): loss=0.4496426589503343\n"
     ]
    }
   ],
   "source": [
    "# Least Squares GD\n",
    "max_iters = 10 # max iter number\n",
    "gamma = 3e-37 # gradient descent speed , not used here as gamma updated by backtracking\n",
    "tX_final_gd = im.build_poly(tX_final, 5)\n",
    "initial_w = np.zeros(tX_final_gd.shape[1]) # initialisation of initial weights\n",
    "weight, loss = im.least_squares_GD(y, tX_final_gd, initial_w, max_iters, gamma) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(999/999): loss=0.41665152111566617\n",
      "[  3.62608935e-04  -1.04049987e-04  -2.82429019e-05  -2.20764283e-05\n",
      "   1.27468421e-05   1.98604836e-04   8.91084288e-06   2.62891713e-07\n",
      "  -2.23198444e-05  -1.01027861e-04  -2.27677700e-06   1.23174111e-06\n",
      "   1.19884376e-05   1.61961016e-05  -4.48377879e-07   1.70093429e-07\n",
      "  -4.01799785e-05  -5.47652241e-07  -8.02461197e-07  -1.82874726e-05\n",
      "  -8.02105264e-07  -1.37214853e-04  -8.04885199e-07   7.74209473e-05\n",
      "   1.03456434e-04   1.03855121e-04  -1.59245144e-05   1.17985820e-05\n",
      "   1.16181312e-05  -7.70436747e-05]\n"
     ]
    }
   ],
   "source": [
    "# Least Squares SGD\n",
    "initial_w = np.zeros(tX_final.shape[1]) # initialisation of initial weights\n",
    "max_iters = 1000 # max iter number\n",
    "gamma = 1e-8 # gradient descent speed  \n",
    "weight, loss = im.least_squares_SGD(y, tX_final, initial_w, max_iters, gamma)\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  8.03494364e-05  -7.20202264e-03  -6.05417274e-03  -5.47559082e-04\n",
      "  -1.93874699e-02   4.73451612e-04  -2.60379063e-02   3.25106299e-01\n",
      "  -3.80779908e-05  -2.72809161e+00  -2.21220141e-01   9.50794099e-02\n",
      "   6.40351632e-02   2.73635128e+00  -3.31801028e-04  -9.54325157e-04\n",
      "   2.74110803e+00  -5.34165388e-04   9.73499007e-04   3.69225050e-03\n",
      "   3.54487064e-04  -5.43344624e-04  -3.30448035e-01  -1.40800496e-03\n",
      "   8.31432780e-04   1.02117281e-03  -1.68047419e-03  -5.83664806e-03\n",
      "  -1.11088010e-02   2.72855154e+00]\n",
      "0.824241238324\n"
     ]
    }
   ],
   "source": [
    "# Least Squares\n",
    "weight, loss = im.least_squares(y,tX_final)\n",
    "print(weight)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loris/anaconda3/lib/python3.5/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n",
      "/home/loris/anaconda3/lib/python3.5/site-packages/numpy/core/_methods.py:70: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 28]\n",
      "[0, 1, 2, 3, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 28]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]\n",
      "Training RMSE=0.665\n",
      "Training RMSE=0.739\n",
      "Training RMSE=0.678\n",
      "Training RMSE=0.740\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression\n",
    "def ridge_regression_demo(y, tx,lamb,degree):\n",
    "    # define parameter\n",
    "    tX = im.build_poly(tx, degree)\n",
    "    weight, loss =  im.ridge_regression(y , tX , lamb)\n",
    "    \n",
    "    print(\"Training RMSE={tr:.3f}\".format(tr=loss))\n",
    "    return weight ,loss\n",
    "\n",
    "index0 = tX[:,22] == 0\n",
    "tX0 = tX[index0,:]\n",
    "tX0 = np.delete(tX0,22,1)\n",
    "index1 = tX[:,22] == 1\n",
    "tX1 = tX[index1,:]\n",
    "tX1 = np.delete(tX1,22,1)\n",
    "index2 = tX[:,22] == 2\n",
    "tX2 = tX[index2,:]\n",
    "tX2 = np.delete(tX2,22,1)\n",
    "index3 = tX[:,22] == 3\n",
    "tX3 = tX[index3,:]\n",
    "tX3 = np.delete(tX3,22,1)\n",
    "\n",
    "tX0_final, index_final_0 = im.formating(tX0) \n",
    "tX1_final, index_final_1 = im.formating(tX1) \n",
    "tX2_final, index_final_2 = im.formating(tX2) \n",
    "tX3_final, index_final_3 = im.formating(tX3) \n",
    "\n",
    "lambdas = np.logspace(-10,1,10)\n",
    "degrees = [5,6,7,8,9,10,11,12,13,14]\n",
    "\n",
    "lamb0 =1.6681005372e-09\n",
    "degree0 = 6\n",
    "weight0 , loss0 = ridge_regression_demo(y[index0], tX0_final,lamb0,degree0)\n",
    "lamb1 = 7.74263682681e-06\n",
    "degree1 = 10\n",
    "weight1 , loss1 = ridge_regression_demo(y[index1], tX1_final,lamb1,degree1)\n",
    "lamb2 = 2.78255940221e-08\n",
    "degree2 = 11\n",
    "weight2 , loss2 = ridge_regression_demo(y[index2], tX2_final,lamb2,degree2)\n",
    "lamb3 = 4.64158883361e-07\n",
    "degree3 = 11\n",
    "weight3 , loss3  = ridge_regression_demo(y[index3], tX3_final,lamb3,degree3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loris/Documents/EPFL/Master/Machine_Learning/ML_course/Final_PCML/implementations.py:188: RuntimeWarning: overflow encountered in exp\n",
      "  return sum(np.log(1+np.exp(np.dot(tx,w)))-np.multiply(y,(np.dot(tx,w))))\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-1687f92eb52b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0minit_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m931\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mloss\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtX_final\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/loris/Documents/EPFL/Master/Machine_Learning/ML_course/Final_PCML/implementations.py\u001b[0m in \u001b[0;36mlogistic_regression\u001b[1;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[1;31m# get loss and update w.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m         \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearning_by_newton_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m         \u001b[1;31m# log info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/loris/Documents/EPFL/Master/Machine_Learning/ML_course/Final_PCML/implementations.py\u001b[0m in \u001b[0;36mlearning_by_newton_method\u001b[1;34m(y, tx, w, alpha)\u001b[0m\n\u001b[0;32m    215\u001b[0m     \"\"\"\n\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_regression_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m     \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbacktracing_LR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/loris/Documents/EPFL/Master/Machine_Learning/ML_course/Final_PCML/implementations.py\u001b[0m in \u001b[0;36mlogistic_regression_step\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlogistic_regression_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;34m\"\"\"return the loss, gradient.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m     \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/loris/Documents/EPFL/Master/Machine_Learning/ML_course/Final_PCML/implementations.py\u001b[0m in \u001b[0;36mcalculate_loss\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;34m\"\"\"compute the cost by negative log likelihood.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;31m#return  -np.dot(np.ones(y.shape[0]),(y*np.log(sigmoid(np.dot(tx,w)))+(1-y)*np.log((1-sigmoid(np.dot(tx,w))))))/y.shape[0]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcalculate_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "max_iters = 10\n",
    "gamma = 0.001\n",
    "init_w = np.ones(931)\n",
    "w, loss = im.logistic_regression(y, tX_final, init_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
